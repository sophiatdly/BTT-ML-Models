{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Logistic Regression Model From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods:\n",
    "\n",
    "1. An `__init__()` method that takes in an error tolerance as a stopping criterion, as well as max number of iterations.\n",
    "2. A `predict_proba()` method that takes a given matrix of features $X$ and predicts $P = \\dfrac{1}{1+e^{-(X \\cdot W+\\alpha)}}$ for each entry\n",
    "3. A `compute_gradient()` method that computes the gradient vector $G$\n",
    "4. A `compute_hessian()` method that computes the Hessian. Note that the $H$ can be broken down to the following matrix multiplications: $H=(X^T*Q)\\cdot X$. \n",
    "5. An `update_weights()` method that applies gradient descent to update the weights\n",
    "6. A `check_stop()` method that checks whether the model has converged or the max iterations have been met\n",
    "7. A `fit()` method that trains the model. It takes in the data and runs the gradient optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionScratch(object):\n",
    "    \n",
    "    def __init__(self, tolerance = 10**-8, max_iterations = 20):\n",
    "        \n",
    "        self.tolerance = tolerance\n",
    "        self.max_iterations = max_iterations\n",
    "        self.weights_array = None # holds current weights and intercept (intercept is at the last position)\n",
    "        self.prior_w = None # holds previous weights and intercept (intercept is at the last position)\n",
    "        \n",
    "        # once we are done training, these variables will hold the \n",
    "        # final values for the weights and intercept\n",
    "        self.weights = None\n",
    "        self.intercept = None \n",
    "\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        '''\n",
    "        Compute probabilities using the inverse logit\n",
    "        - Inputs: The Nx(K+1) matrix with intercept column X\n",
    "        - Outputs: Vector of probabilities of length N\n",
    "        '''\n",
    "        \n",
    "        XW = np.dot(X, self.weights_array)\n",
    "        P = 1 / (1+np.exp(-XW))\n",
    "        \n",
    "        \n",
    "        return P\n",
    "\n",
    "    \n",
    "    \n",
    "    def compute_gradient(self, X, y, P):\n",
    "        '''\n",
    "        Computes the gradient vector\n",
    "        -Inputs:\n",
    "            - The Nx(K+1) matrix with intercept column X\n",
    "            - Nx1 vector y (label) \n",
    "            - Nx1 vector of predictions P\n",
    "        -Outputs: 1x(K+1) vector of gradients\n",
    "        '''\n",
    "        \n",
    "        G = -np.dot(X.T, (y-P))\n",
    "\n",
    "                \n",
    "        return G\n",
    "        \n",
    "    def compute_hessian(self, X, P):\n",
    "        '''\n",
    "        computes the Hessian matrix\n",
    "        -inputs:\n",
    "            - Nx(K+1) matrix X\n",
    "            - Nx1 vector of predictions P\n",
    "        -outputs:\n",
    "            - KxK Hessian matrix H=X^T * Diag(Q) * X\n",
    "        '''\n",
    "\n",
    "        Q = P*(1-P)\n",
    "        XQ = X.T * Q\n",
    "        H = np.dot(XQ, X)\n",
    "        \n",
    "        return H\n",
    "\n",
    "\n",
    "    def update_weights(self, X, y):\n",
    "        '''\n",
    "        Updates existing weight vector\n",
    "        -Inputs:\n",
    "            -Nx(Kx1) matrix X\n",
    "            -Nx1 vector y\n",
    "        -Calls predict_proba, compute_gradient and compute_hessian and uses the \n",
    "        return values to update the weights array\n",
    "        '''\n",
    "        \n",
    "        P = self.predict_proba(X)\n",
    "        G = self.compute_gradient(X, y, P)\n",
    "        H = self.compute_hessian(X, P)\n",
    "        self.prior_w = np.copy(self.weights_array)\n",
    "        H_inv = np.linalg.inv(H)\n",
    "        update_step = np.dot(H_inv, G)\n",
    "        self.weights_array-=update_step\n",
    "        \n",
    "      \n",
    "           \n",
    "    def check_stop(self):\n",
    "        '''\n",
    "        check to see if euclidean distance between old and new weights (normalized)\n",
    "        is less than the tolerance\n",
    "        \n",
    "        returns: True or False on whether stopping criteria is met\n",
    "        '''\n",
    "        \n",
    "        w_old_norm = self.prior_w / np.linalg.norm(self.prior_w)\n",
    "        w_new_norm = self.weights_array / np.linalg.norm(self.weights_array)\n",
    "        diff = w_old_norm - w_new_norm\n",
    "        distance = np.sqrt(np.dot(diff, diff))\n",
    "        stop = distance < self.tolerance\n",
    "        \n",
    "        \n",
    "        return stop\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        X is the Nx(K-1) data matrix\n",
    "        Y is the labels, using {0,1} coding\n",
    "        '''\n",
    "        \n",
    "        #set initial weights - add an extra dimension for the intercept\n",
    "        self.weights_array = np.zeros(X.shape[1] + 1)\n",
    "        \n",
    "        #Initialize the slope parameter to log(base rate/(1-base rate))\n",
    "        self.weights_array[-1] = np.log(y.mean() / (1-y.mean()))\n",
    "        \n",
    "        #create a new X matrix that includes a column of ones for the intercept\n",
    "        X_int = np.hstack((X, np.ones((X.shape[0],1))))\n",
    "\n",
    "        for i in range(self.max_iterations):\n",
    "            self.update_weights(X_int, y)\n",
    "            \n",
    "            # check whether we should\n",
    "            stop = self.check_stop()\n",
    "            if stop:\n",
    "                # since we are stopping, lets save the final weights and intercept\n",
    "                self.set_final_weights()\n",
    "                self.set_final_intercept()\n",
    "                break\n",
    "                \n",
    "    \n",
    "    def set_final_weights(self):\n",
    "        self.weights = self.weights_array[0:-1]\n",
    "        \n",
    "    def set_final_intercept(self):\n",
    "        self.intercept = self.weights_array[-1]  \n",
    "        \n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "    \n",
    "    def get_intercept(self):\n",
    "        return self.intercept\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the Class to Train a Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(os.getcwd(), \"data\", \"airbnbData_train.csv\")\n",
    "df = pd.read_csv(filename, header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['review_scores_rating',\n",
       " 'review_scores_cleanliness',\n",
       " 'review_scores_checkin',\n",
       " 'review_scores_communication',\n",
       " 'review_scores_value',\n",
       " 'host_response_rate',\n",
       " 'host_acceptance_rate']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_list = ['review_scores_rating','review_scores_cleanliness','review_scores_checkin','review_scores_communication','review_scores_value','host_response_rate','host_acceptance_rate']\n",
    "feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"host_is_superhost\"]\n",
    "X = df[feature_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegressionScratch()\n",
    "lr.fit(X, y)\n",
    "print('The fitted weights and intercept are:')\n",
    "print(lr.get_weights(), lr.get_intercept())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare with Scikit-Learn's Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10000000000, class_weight=None, dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_sk = LogisticRegression(C=10**10)\n",
    "lr_sk.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fitted weights and intercept with sklearn are:\n",
      "[[ 0.56691547  0.49224905  0.20150113  0.25563246 -0.005929    1.71592022\n",
      "   0.26479199]] [-1.82906726]\n"
     ]
    }
   ],
   "source": [
    "print('The fitted weights and intercept with sklearn are:')\n",
    "print(lr_sk.coef_, lr_sk.intercept_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 ms ± 4.53 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# %timeit to fit the logistic model lr on the training data\n",
    "%timeit lr.fit(X, y)\n",
    "\n",
    "# %timeit to fit the logistic model lr_sk on the training data\n",
    "%timeit lr_sk.fit(X, y)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
